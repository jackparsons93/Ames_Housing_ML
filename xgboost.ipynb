{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aef8968-8be6-41a3-a6de-2c72e1ee5fce",
   "metadata": {},
   "source": [
    "# Below is my best score of .95 on xgboost, gotten after 50 attempts at hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be0a594-e6cc-44b9-ae7f-1b452f0e77ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriswilson/Desktop/ames/tf/lib/python3.11/site-packages/sklearn/impute/_base.py:598: UserWarning: Skipping features without any observed values: ['MA_Line2' 'X2TPr_D' 'X2TSc_D' 'X2TPr_S' 'X2TSc_S' 'PA-PostD']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Lasso with alpha=0.001\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best XGBoost Parameters for alpha=0.001: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "XGBoost Model Mean Squared Error for alpha=0.001: 320937885.95468134\n",
      "XGBoost Model R-squared for alpha=0.001: 0.9460580348968506\n",
      "Running Lasso with alpha=0.01\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best XGBoost Parameters for alpha=0.01: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "XGBoost Model Mean Squared Error for alpha=0.01: 309527158.7336001\n",
      "XGBoost Model R-squared for alpha=0.01: 0.9479759335517883\n",
      "Running Lasso with alpha=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriswilson/Desktop/ames/tf/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.616e+09, tolerance: 1.178e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best XGBoost Parameters for alpha=0.1: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "XGBoost Model Mean Squared Error for alpha=0.1: 303793238.5887984\n",
      "XGBoost Model R-squared for alpha=0.1: 0.9489396810531616\n",
      "Running Lasso with alpha=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriswilson/Desktop/ames/tf/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.019e+10, tolerance: 1.178e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best XGBoost Parameters for alpha=1: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "XGBoost Model Mean Squared Error for alpha=1: 294400067.5396449\n",
      "XGBoost Model R-squared for alpha=1: 0.9505184292793274\n",
      "Alpha: 0.001\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "MSE: 320937885.95468134\n",
      "R2: 0.9460580348968506\n",
      "\n",
      "\n",
      "Alpha: 0.01\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "MSE: 309527158.7336001\n",
      "R2: 0.9479759335517883\n",
      "\n",
      "\n",
      "Alpha: 0.1\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "MSE: 303793238.5887984\n",
      "R2: 0.9489396810531616\n",
      "\n",
      "\n",
      "Alpha: 1\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 7000, 'subsample': 0.8}\n",
      "MSE: 294400067.5396449\n",
      "R2: 0.9505184292793274\n",
      "\n",
      "\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.001, max_depth=3, n_estimators=7000, subsample=0.8; total time= 2.3min\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the functions from the module\n",
    "from data_processing_module import load_data, preprocess_data, split_data\n",
    "\n",
    "# Cell 2: Use the imported functions\n",
    "file_path = 'merged3.csv'  # Update this path to the location of your file\n",
    "target_column = 'SalePrice_x'\n",
    "drop_columns = ['Unnamed: 0', 'PID', 'Index']\n",
    "\n",
    "# Load the data\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "X_preprocessed, y, preprocessor = preprocess_data(data, target_column, drop_columns)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = split_data(X_preprocessed, y)\n",
    "\n",
    "# Define the alpha values for Lasso regression\n",
    "alpha_values = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [7000],\n",
    "    'learning_rate': [0.03, 0.001],\n",
    "    'max_depth': [3],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.9]\n",
    "}\n",
    "\n",
    "# Initialize the results dictionary\n",
    "results = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    print(f\"Running Lasso with alpha={alpha}\")\n",
    "\n",
    "    # Define and fit the Lasso model\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "    # Define the XGBoost model\n",
    "    xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "    # Hyperparameter tune the XGBoost model\n",
    "    grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, scoring='r2', cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "    best_xgb = grid_search_xgb.best_estimator_\n",
    "    print(f\"Best XGBoost Parameters for alpha={alpha}: {grid_search_xgb.best_params_}\")\n",
    "\n",
    "    # Make predictions with the best XGBoost model\n",
    "    y_pred_xgb = best_xgb.predict(X_test_selected)\n",
    "\n",
    "    # Evaluate the XGBoost model\n",
    "    mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "    print(f'XGBoost Model Mean Squared Error for alpha={alpha}: {mse_xgb}')\n",
    "    print(f'XGBoost Model R-squared for alpha={alpha}: {r2_xgb}')\n",
    "\n",
    "    # Store the results\n",
    "    results[alpha] = {\n",
    "        'best_params': grid_search_xgb.best_params_,\n",
    "        'mse': mse_xgb,\n",
    "        'r2': r2_xgb\n",
    "    }\n",
    "\n",
    "# Optionally, you can plot the results or further analyze them\n",
    "for alpha, result in results.items():\n",
    "    print(f\"Alpha: {alpha}\")\n",
    "    print(f\"Best Parameters: {result['best_params']}\")\n",
    "    print(f\"MSE: {result['mse']}\")\n",
    "    print(f\"R2: {result['r2']}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
